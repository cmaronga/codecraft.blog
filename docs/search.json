[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Christopher Maronga",
    "section": "",
    "text": "My name is Christopher Maronga, I hail from the beautiful landscapes of Kenya, East Africa.\nI have a wealth of experience managing and analysing healthcare data for decision-making, as well as teaching introductory and advanced courses in R.\nI’m passionate about reproducible research, automating data products, statistical computing, and data visualization."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "Christopher Maronga",
    "section": "",
    "text": "My name is Christopher Maronga, I hail from the beautiful landscapes of Kenya, East Africa.\nI have a wealth of experience managing and analysing healthcare data for decision-making, as well as teaching introductory and advanced courses in R.\nI’m passionate about reproducible research, automating data products, statistical computing, and data visualization."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Christopher Maronga",
    "section": "Education",
    "text": "Education\n\nMSc. Statistical Sciences |Strathmore University\nBSc. in Mathematics |JKUAT"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Christopher Maronga",
    "section": "Experience",
    "text": "Experience\n\nPhase II & IV CTs\nReal World Data/Real World Evidence\nHealth Research\nData Management/Wrangling\nData Analysis/Statistical Analysis\nPredictive Modelling\nHealth Economics"
  },
  {
    "objectID": "about.html#skillslanguages",
    "href": "about.html#skillslanguages",
    "title": "Christopher Maronga",
    "section": "Skills/languages",
    "text": "Skills/languages\n\nR Programming/RShiny\nPython\nSQL/MySQL\nREDCap\nGit/GitHub\nLaTeX\nDocker"
  },
  {
    "objectID": "posts/automating-data-management/index.html",
    "href": "posts/automating-data-management/index.html",
    "title": "Automating Clinical Data Management",
    "section": "",
    "text": "Using REDCap, MySQL, RShiny-server"
  },
  {
    "objectID": "posts/automating-data-management/index.html#introduction",
    "href": "posts/automating-data-management/index.html#introduction",
    "title": "Automating Clinical Data Management",
    "section": "Introduction",
    "text": "Introduction\nData management in multi-centre cohort studies presents unique challenges, particularly when these study sites are widely dispersed geographically and possess varying levels of technical and human resources. It is crucial to maintain data quality, promptly address data inconsistencies, and generate timely progress reports. This is especially true for initiatives like the Childhood Acute Illness and Nutrition Network (CHAIN), which involved recruiting participants from nine hospitals located in Africa and Asia. The CHAIN study collected highly detailed data spanning various biological and social domains."
  },
  {
    "objectID": "posts/automating-data-management/index.html#methods",
    "href": "posts/automating-data-management/index.html#methods",
    "title": "Automating Clinical Data Management",
    "section": "Methods",
    "text": "Methods\nTo streamline data management processes, the CHAIN cohort employed Research Electronic Data Capture (REDCap), a web-based, open-source database driven by metadata. In this article, I describe our effective utilization of REDCap in combination with the open-source R software to automate the data management workflow for the CHAIN study. I aim to document, step by step, the various decisions and implementations that were instrumental in this process. I plan to make this information available on my webpage for others to read and implement, although this will require a few days to compile and potentially create informative infographics.\nAdditionally, I had the opportunity to present this work at the Why R conference in 2021, and you can watch the presentation on YouTube. The presentation slides are also accessible here for download\nI hope after watching the conference presentation, you shall an idea how this was set up. I am working toward presenting this work as a guideline, untill then, pleas be patient and if you have any questions, feel free to reach me via e-mai."
  },
  {
    "objectID": "posts/working-with-databases/index.html",
    "href": "posts/working-with-databases/index.html",
    "title": "Working with Databases in R",
    "section": "",
    "text": "MySQL, SQLite, R\nThis blog post comes as a follow-up to a successful online training session jointly organized by NairobiR and RLadies on June 12, 2021. If you missed the session, you can view the recording on this YouTube link\nIn both the video and this accompanying blog, I delve into the fundamental principles of establishing dynamic connections and extracting data from databases using Application Programming Interfaces (APIs). Regardless of how and where data is stored, the initial step in any data management process involves loading it into your preferred working tool.\nRelational databases, such as RMySQL, and web-based databases, like REDCap, have gained popularity for efficiently and cost-effectively managing small to medium-sized datasets. In this blog, I’ll guide you through the essential steps to access and utilize data stored in these platforms using the R statistical language. I will provide practical examples to demonstrate:\n\nEstablishing an efficient connection between R and Relational Database Management Systems (RDBMS).\nQuerying data housed in any RDBMS directly from within R/RStudio.\nConnecting to and querying data from a Research Electronic Data Capture (REDCap) database.\nBest practices for securing your API while collaborating on projects.\n\nPlease note that this blog is a work in progress. In the meantime, you can access the workshop slides for reference here and the youtube link should be resourceful since this was a live coding exercise.\nI also delivered a similar workshop at NHS-R conference in 2022, the workshop materials can be found here, while the recorded workshop video can be accessed in this YouTube link."
  },
  {
    "objectID": "posts/web-scraping-in-R/index.html",
    "href": "posts/web-scraping-in-R/index.html",
    "title": "Web scraping using R",
    "section": "",
    "text": "Efficient, flexible and powerful!"
  },
  {
    "objectID": "posts/web-scraping-in-R/index.html#dollar-exchange-data",
    "href": "posts/web-scraping-in-R/index.html#dollar-exchange-data",
    "title": "Web scraping using R",
    "section": "Dollar exchange data",
    "text": "Dollar exchange data\nI came across this website Exchange rate to USD by country while I was working on aggregating output from aneconomic micro-simulation model estimating benefits and budget impact of setting up fracture liaison services. Part of my reporting costs associated with the FLS, reported in local currency for about 10 different countries, but for harmonized reporting and international consumption, I need to convert the local currencies to US Dollars, hence I needed a source that I could cite and is updated regularly.\n\n# get url from website\nexchange_rate_url &lt;- \"https://www.theglobaleconomy.com/rankings/Dollar_exchange_rate/\"\n\n# read HTML from website\nexchange_rate_webpage &lt;- read_html(exchange_rate_url)       # read html site\n\n\n# create a datframe containing the exchange rates for use\ncurrency_rates &lt;- exchange_rate_webpage %&gt;%\n  html_table() %&gt;%                                          # output a list contain the exchange rate table from website\n  as.data.frame() %&gt;%                                       # transform into a tibble\n  # rename columns appropriately\n  rename(\n    Country        = Countries,\n    DateLatestData     = \"Reference\",\n    LatesValue     = \"Latest.value\",\n    Change3months  = \"Change.three.months\",\n    Change12months = \"Change.twelve.months\"\n  ) %&gt;%\n  mutate(across(LatesValue, as.numeric),                  # convert to numeric column containing exchange rate\n         \n         # On the date column; remove white space and replace \"/\" with \"-\"\n         DateLatestData = str_squish(\n           str_replace_all(\n             string = DateLatestData,\n             pattern = \"/\",\n             replacement = \"-\"\n           )\n         )) %&gt;% \n  # transform `DateLatestData` into a proper date column\n  mutate(\n    DateLatestData = my(DateLatestData),                       # proper date format\n    DateLatestData = format(DateLatestData, \"%Y-%m\")           # YYYY-mm\n  )\n\nThe first 15 rows of the harvested data are show below\n\ncurrency_rates %&gt;% \n  head(15)\n\n               Country LatesValue DateLatestData Change3months Change12months\n1          Afghanistan    70.7133        2024-08        -0.61%        -17.45%\n2              Albania    90.7575        2024-08        -2.10%         -6.33%\n3              Algeria   134.2240        2024-08        -0.18%         -1.35%\n4              Andorra     0.9073        2024-08        -1.68%         -1.03%\n5               Angola   891.8101        2024-08         5.40%          7.95%\n6  Antigua and Barbuda     2.7026        2024-08         0.00%          0.00%\n7            Argentina   940.2254        2024-08         5.86%        195.41%\n8              Armenia   386.9613        2024-08         0.03%          0.33%\n9                Aruba     1.8006        2024-08        -0.04%         -0.03%\n10           Australia     1.5030        2024-08        -0.15%         -2.62%\n11             Austria     0.9073        2024-08        -1.68%         -1.03%\n12          Azerbaijan     1.7000        2024-08         0.00%          0.00%\n13             Bahamas     0.9986        2024-08        -0.07%         -0.09%\n14             Bahrain     0.3766        2024-08         0.00%         -0.03%\n15          Bangladesh   118.1472        2024-08         0.91%          8.15%\n\n\nYou can now export the harvested data for later use downstream or in your modelling framework like I did. You see, easy and straight forward, right?\n\n# Export the data\n\nrio::export(currency_rates, here(\"datasets\", \"dollar_exchange_rates.rds\"))"
  },
  {
    "objectID": "posts/web-scraping-in-R/index.html#available-cran-packages",
    "href": "posts/web-scraping-in-R/index.html#available-cran-packages",
    "title": "Web scraping using R",
    "section": "Available CRAN packages",
    "text": "Available CRAN packages\nIn this section, we are going to harvest data from two websites:-\n\nList of available CRAN packages by data of publication\nTracked website of CRAN packages by number of downloads\nAdditionally, to get some metrics to combine with data from the above 2 sites, we I use available.packages function for all cran packages in this mirror\n\n\nCRAN packages by publication date\nI will use the 3 simple steps already covered earlier, employing rvest package.\n\n# Specifying the url\nurl &lt;- 'https://cran.r-project.org/web/packages/available_packages_by_date.html'\n\n\n# create a dataframe using `rvest` functions\nr_pkgs_by_date &lt;- read_html(url) %&gt;% \n  html_table() %&gt;% \n  as.data.frame() %&gt;% \n  mutate(\n    Date = ymd(Date)\n  ) %&gt;% \n  rename(pkg_name = Package, date_published = Date)\n\n# view top n rows\nr_pkgs_by_date %&gt;% \n  head(10)\n\n   date_published    pkg_name\n1      2024-09-08    datefixR\n2      2024-09-08   EMCluster\n3      2024-09-08     ibdsim2\n4      2024-09-08      IsoriX\n5      2024-09-08      L1pack\n6      2024-09-08 matrixStats\n7      2024-09-08         msm\n8      2024-09-08   openxlsx2\n9      2024-09-08    pedtools\n10     2024-09-08   PMCMRplus\n                                                                              Title\n1                       Standardize Dates in Different Formats or with Missing Data\n2  EM Algorithm for Model-Based Clustering of Finite Mixture\\nGaussian Distribution\n3                        Simulation of Chromosomal Regions Shared by Family Members\n4         Isoscape Computation and Inference of Spatial Origins using\\nMixed Models\n5                                                        Routines for L1 Estimation\n6            Functions that Apply to Rows and Columns of Matrices (and to\\nVectors)\n7                    Multi-State Markov and Hidden Markov Models in Continuous Time\n8                                                 Read, Write and Edit 'xlsx' Files\n9                               Creating and Working with Pedigrees and Marker Data\n10              Calculate Pairwise Multiple Comparisons of Mean Rank Sums\\nExtended\n\n\n\n\nCRAN packages metadata\nIf I need additional details, I can use the function available.packages(); downside, it doesn’t contain date of publication, but some other metrics which are equally important are returned by this function. So, I ma going to do that below.\n\n# using available.packages function\n\navailable_pks &lt;- available.packages(#repos = \"http://cran.us.r-project.org\",\n                                                                            # specify CRAN mirror and metadata to extract\n  repos = \"https://cran.r-project.org/\")[, c(\"Version\",\n                                             \"Depends\",\n                                             \"Repository\",\n                                             \"NeedsCompilation\",\n                                             \"License\")] %&gt;%\n  as.data.frame() %&gt;% tibble::rownames_to_column(var = \"pkg_name\")\n\n# view top n rows\navailable_pks %&gt;% \n  head(10)\n\n        pkg_name Version                                             Depends\n1             A3   1.0.0                      R (&gt;= 2.15.0), xtable, pbapply\n2  AalenJohansen     1.0                                                &lt;NA&gt;\n3       AATtools   0.0.3                                        R (&gt;= 3.6.0)\n4         ABACUS   1.0.0                                        R (&gt;= 3.1.0)\n5    abasequence   0.1.0                                                &lt;NA&gt;\n6     abbreviate     0.1                                                &lt;NA&gt;\n7            abc   2.2.1 R (&gt;= 2.10), abc.data, nnet, quantreg, MASS, locfit\n8       abc.data     1.1                                         R (&gt;= 2.10)\n9        ABC.RAP   0.9.0                                        R (&gt;= 3.1.0)\n10   ABCanalysis   1.2.1                                         R (&gt;= 2.10)\n                               Repository NeedsCompilation    License\n1  https://cran.r-project.org/src/contrib               no GPL (&gt;= 2)\n2  https://cran.r-project.org/src/contrib               no GPL (&gt;= 2)\n3  https://cran.r-project.org/src/contrib               no      GPL-3\n4  https://cran.r-project.org/src/contrib               no      GPL-3\n5  https://cran.r-project.org/src/contrib               no      GPL-3\n6  https://cran.r-project.org/src/contrib               no      GPL-3\n7  https://cran.r-project.org/src/contrib               no GPL (&gt;= 3)\n8  https://cran.r-project.org/src/contrib               no GPL (&gt;= 3)\n9  https://cran.r-project.org/src/contrib               no      GPL-3\n10 https://cran.r-project.org/src/contrib               no      GPL-3\n\n\n\n\nCRAN packages downloads\nWe can grab metric on number of downloads for each package in CRAN from this website.\n\npkg_down_url &lt;- \"https://www.datasciencemeta.com/rpackages\"\n\npkg_downloads &lt;- read_html(pkg_down_url) %&gt;% \n  html_table() %&gt;% \n  as.data.frame() %&gt;% \n  select(-c(Author, Maintainer)) %&gt;%                          # remove columns with no data\n  mutate(\n    Downloads = str_remove_all(Downloads, pattern = \",\"),     # get rid of commas\n    Downloads = as.integer(Downloads)                         # convert into an integer\n  ) %&gt;% \n  rename(pkg_name = \"Package.Name\", downloads = Downloads)\n\n# view top n rows\npkg_downloads %&gt;% \n  head(10)\n\n   Rank pkg_name downloads\n1     1  ggplot2 147979440\n2     2    rlang 136425392\n3     3 magrittr 126139801\n4     4    dplyr 111533228\n5     5    vctrs  99097308\n6     6      cli  96527305\n7     7   tibble  93773928\n8     8 devtools  91913477\n9     9 jsonlite  91716968\n10   10     Rcpp  88506647\n\n\nHere is the fun part, we can now join all the three datasets telling different aspects of the packages to have one dataframe we can use for visualization and expropriation, how cool is that🙂\n\nCRAN_pkgs &lt;- reduce(\n  list(r_pkgs_by_date, pkg_downloads, available_pks), # reduce from purrr package\n  left_join,\n  by = \"pkg_name\"                                     # join by package_name\n) %&gt;% arrange(Rank) %&gt;% \n  select(Rank, pkg_name, downloads, everything())\n\n# view top n rows\n\nCRAN_pkgs %&gt;% \n  head(5) %&gt;% \n  reactable::reactable()\n\n\n\n\n\n\nFor instance, as of the harvesting of this data, what are the top 20 most downloaded packages from CRAN. We can be able to summarize and visualize that information\n\nCRAN_pkgs %&gt;% \n  filter(Rank &lt;= 20) %&gt;% \n  reactable::reactable()\n\n\n\n\n\n\nMore fun exploration and data wrangling can be performed on the harvested data, feel free to play with the data further.\nExport the combined dataset\n\n# export data for playing around with\n\nrio::export(CRAN_pkgs, here(\"datasets\", \"CRAN_pkgs.rds\"))"
  },
  {
    "objectID": "posts/coalesce-in-action/index.html",
    "href": "posts/coalesce-in-action/index.html",
    "title": "dplyr::coalesce() in action",
    "section": "",
    "text": "Viable shortcut to missing-value-free data set"
  },
  {
    "objectID": "posts/coalesce-in-action/index.html#introduction",
    "href": "posts/coalesce-in-action/index.html#introduction",
    "title": "dplyr::coalesce() in action",
    "section": "Introduction",
    "text": "Introduction\nMissing data can be a major headache when handling data in R; more especially if these NA’s are scattered across multiple columns representing your one variable of interest.\nThe coalesce() function from dplyr package comes in handy in such situations. In this short article, I am going to show you how powerful the function coalesce() is and how it can make your data manipulation task easy. To put in perspective, the function extracts the first non-missing (non-NA) value from a set of columns or values, making your data cleaner and analysis-ready, it does this by prioritizing a designated column as we will see from the worked out examples below."
  },
  {
    "objectID": "posts/coalesce-in-action/index.html#create-fictious-data",
    "href": "posts/coalesce-in-action/index.html#create-fictious-data",
    "title": "dplyr::coalesce() in action",
    "section": "Create fictious data",
    "text": "Create fictious data\n\n# load packages\npacman::p_load(\n  tidyverse,\n  flextable\n)\n\n# Create fictitious data\nfict_data &lt;- tribble(\n ~PTID, ~ HT_ADM, ~ HT_DISCH, ~ HT_D45, ~ HT_D90,       # patient ID and Height measurements at different time-points\n  \"7001\",    100,      100.1,        100,       NA_real_,\n  \"7002\",    97,       98,         NA_real_,  97.5,\n  \"7003\",    97.6,     NA_real_,    97,      97.5,\n  \"7004\",    NA_real_,  99,        99.8,     100,\n  \"7005\",    79.5,  NA_real_,   78.9,     NA_real_,\n  \"7006\",    NA_real_,  NA_real_,   NA_real_,  102.1,\n  \"7007\",    78.5,      79,         NA_real_,    NA_real_,\n  \"7008\",    NA_real_,      98,        98,       NA_real_)\n\n\n# qflextable-it\nfict_data %&gt;% \n  qflextable()\n\n\n\nTable 1:  Ficticious patient data PTIDHT_ADMHT_DISCHHT_D45HT_D907001100.0100.1100.0700297.098.097.5700397.697.097.5700499.099.8100.0700579.578.97006102.1700778.579.0700898.098.0\n\n\n\nThe data in Table 1 is a made up one, representing height measurements of 8 patients across 4 time-points i.e during admission, discharge, day 45 and day 90 follow up visits respectively. As you can see, there is no single column with complete data. The assumption is that the measurement are subject to some absolute error and therefore any column can be used as measured height in an analysis; the problem is that no column is complete and this is what we are going to tackle using coalesce()"
  },
  {
    "objectID": "posts/coalesce-in-action/index.html#creating-height-column",
    "href": "posts/coalesce-in-action/index.html#creating-height-column",
    "title": "dplyr::coalesce() in action",
    "section": "Creating Height column",
    "text": "Creating Height column\nThe aim is to use coalesce() to create a column HT_cm with non-missing data using. We can try a few combination, but I am confident that if you look carefully, there are probably a few combination of two columns that can give you that.\nNOTE: This is an illustration of how coalesce() works, your use-case might be different, just think how this function fits in.\n\nHT_ADM vs HT_D90\nHere coalesce() gives priority to HT_ADM and if this column has some NA’s while HT_D90 has values in the same row; the values of HT_D90 get pulled/filled in place of NA’s to form HT_CM.\n\nfict_data %&gt;% \n  mutate(\n    HT_CM = coalesce(HT_ADM, HT_D90)    # prioritize _ADM over _D90\n  ) %&gt;% \n  flextable()\n\n\nPTIDHT_ADMHT_DISCHHT_D45HT_D90HT_CM7001100.0100.1100.0100.0700297.098.097.597.0700397.697.097.597.6700499.099.8100.0100.0700579.578.979.57006102.1102.1700778.579.078.5700898.098.0\n\n\nBut we still have one more missing value for patient 7008\n\n\nHT_ADM vs HT_DISH\nTake a combination of measurement at admission and discharge for instance.\n\nfict_data %&gt;% \n  mutate(\n    HT_CM = coalesce(HT_ADM, HT_DISCH) # prioritize _ADM over _DISCH\n  ) %&gt;% \n  flextable()\n\n\nPTIDHT_ADMHT_DISCHHT_D45HT_D90HT_CM7001100.0100.1100.0100.0700297.098.097.597.0700397.697.097.597.6700499.099.8100.099.0700579.578.979.57006102.1700778.579.078.5700898.098.098.0\n\n\nSame problem, we still have a missing cell for patient 7006 on the HT_CM column\n\n\nHT_D90 vs HT_DISCH\nWe can reverse the natural order of the visits and the logic still remains. In the below example, coalesce() prioritizes HT_D90 over HT_DISCH but we still end up a missing cell in patient 7005 since both measurements are NA at discharge and at day 90.\n\nfict_data %&gt;% \n  mutate(\n    HT_CM = coalesce(HT_D90, HT_DISCH)   # prioritize _D90 over _DISCH\n  ) %&gt;%  \n  flextable()\n\n\nPTIDHT_ADMHT_DISCHHT_D45HT_D90HT_CM7001100.0100.1100.0100.1700297.098.097.597.5700397.697.097.597.5700499.099.8100.0100.0700579.578.97006102.1102.1700778.579.079.0700898.098.098.0\n\n\n\n\nUse all columns\nThe ideal is to use all the columns. coalesce() prioritizes form “left to right” (in this case HT_ADM) filling in the missing cells on the left with the first non-missing value it encounters to the right. This is how it takes care of the non-missing business in your data.\n\nfict_data %&gt;% \n  mutate(\n    HT_CM = coalesce(HT_ADM, HT_DISCH, HT_D45, HT_D90)\n  ) %&gt;% \n  flextable()\n\n\nPTIDHT_ADMHT_DISCHHT_D45HT_D90HT_CM7001100.0100.1100.0100.0700297.098.097.597.0700397.697.097.597.6700499.099.8100.099.0700579.578.979.57006102.1102.1700778.579.078.5700898.098.098.0"
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html",
    "href": "posts/celebrate-100-gym-days/index.html",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "",
    "text": "Because I can exercise and share a brief story!"
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html#introduction",
    "href": "posts/celebrate-100-gym-days/index.html#introduction",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "Introduction",
    "text": "Introduction\nWelcome to the journey of celebrating 100 days of unwavering dedication to my fitness regimen. Beginning of the year, I embarked on a quest for personal betterment, focusing on gym weight-lifting and outdoor running exercises, with a target of achieving at least 11 kilometers each weekend (fun fact: I exceeded my goal by running approximately 28 kilometers in just about two weekends).\nThis journey culminated in a staggering 100 days of sheer hard work, unwavering dedication, and a growing passion for all things fitness. In this article, I aim to take you through this transformative 100-day journey, sharing the challenges I overcame, and the invaluable lessons I’ve learned along the way.\n\n# Load packages for session\npacman::p_load(\n  lubridate,   # handle and manage dates\n  flextable,   # for quick and neat tables\n  ggthemes,    # ggplot2 themes extension\n  here,        # manage file paths\n  rio,         # data import\n  scales,      # for the date label function\n  tidyverse    # data wrangling and viz\n)\n\n# set theme for ggplot2\ntheme_set(\n  theme_hc(base_size = 18) +                  # base size\n    theme(\n      legend.position = \"none\",              # No legend, unless specified in the plot\n      legend.title = element_blank(),         # No title on the legend\n      axis.title.y = element_text(angle = 90) # spin y title 90 degrees\n    )\n)\n\nThe below code below utilize the tribble function to create row-wise tibble of the workout data for the months beggining January 2023 to my celebration month and date.\nI also import an excel log (from the gym logs) capturing start and end times for all the gym workouts for the months listed. This will be used to create interesting visualization later on.\n\n# Creating tibble for monthly workout counts\n# used to visualize in the latter sections\nworkout_df &lt;- tribble(\n  ~ month, ~n_workouts,\n  \"2023-01\", 14 ,\n  \"2023-02\", 13,\n  \"2023-03\",05 ,\n  \"2023-04\", 0,\n  \"2023-05\",02 ,\n  \"2023-06\",14 ,\n  \"2023-07\", 23,\n  \"2023-08\", 15,\n  \"2023-09\",0 ,\n  \"2023-10\", 14\n)\n\n# import gym logs\ngym_log_raw &lt;- import(here(\"posts\",\"celebrate-100-gym-days\", \"chris-gym_game.xlsx\"))\n\n# data processing\nworkout_df &lt;- workout_df %&gt;% \n  mutate(\n    month = ym(month),\n    n_workouts = as.integer(n_workouts)\n  )"
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html#cleaning-gym-logs-data",
    "href": "posts/celebrate-100-gym-days/index.html#cleaning-gym-logs-data",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "Cleaning gym logs data",
    "text": "Cleaning gym logs data\n\n# clean the gym log data\ngym_log &lt;- gym_log_raw %&gt;% \n  \n  # create visit date, stat time and end time\n  mutate(\n    visit_start_time  = str_extract(visit_start, \"\\\\d+:\\\\d+[aApP][mM]\"),       # extract time component\n    visit_end_time    = str_extract(visit_end, \"\\\\d+:\\\\d+[aApP][mM]\"),         # extract time component\n    visit_date        = str_extract(visit_start, \"\\\\d{2} [A-Za-z]{3} \\\\d{2}\"), # extract date component\n    visit_date        = dmy(visit_date),                                       # convert into ISO date\n    week_day          = wday(visit_date, label = T, abbr = T),                 # day of the week\n    month             = month(visit_date, label = T),                          # workout month\n    duration_mins     = as.integer(str_remove(duration, \" minutes\")),          # minutes in the gym\n    morning_wkout     = ifelse(str_detect(visit_start, \"am\"), \"Yes\", \"No\")     # Morning workout (Yes/No)\n  ) %&gt;% \n  \n  # drop the first 3 columns, not neede downstream\n  select(-c(1:3)) %&gt;% \n  arrange(visit_date)\n\ngym_log %&gt;% select(-gym_game) %&gt;% head(n = 8) %&gt;% \n  flextable()\n\n\nvisit_start_timevisit_end_timevisit_dateweek_daymonthduration_minsmorning_wkout10:47am11:40am2023-01-02MonJan53Yes4:56pm6:12pm2023-01-03TueJan76No5:21pm6:37pm2023-01-04WedJan76No1:49pm3:26pm2023-01-08SunJan97No8:11pm9:23pm2023-01-09MonJan72No8:16pm9:28pm2023-01-10TueJan72No7:45pm9:04pm2023-01-11WedJan79No6:22am7:48am2023-01-13FriJan86Yes\n\n\nMy journey commenced with setting realistic and achievable fitness goals, providing me with a clear path and unwavering focus. My objectives included:-\n\nEnhancing full-body muscle strength\nBuild endurance\nOverall fitness\n\nMy workout routine was marked by monthly fluctuations, and I visualized these trends to identify both my best and most challenging months.\n\nworkout_df %&gt;%\n  ggplot(aes(x = month, y = n_workouts, group = 1)) +\n  geom_point(aes(size = n_workouts)) +   # size of point depends on number of workouts\n  geom_line(linewidth = 0.8) +\n  labs(title = \"Trend of monthly gym workouts\",\n       subtitle = \"Past 10 months only\",\n       x = \"Month\", y = \"No. of workouts\") +\n  scale_x_date(date_breaks = \"month\",    # break by 1 month interval\n               date_labels = \"%b\")       # show only month of the year\n\n\n\n\nFigure 1: Monthly Gym workouts\n\n\n\n\nFigure 1 shows a trend of monthly workouts, with my worst efforts in the months of March and May (I have an excuse though 😂). I reached by best efforts in the month of July with a total of 23 gym works in that monthly only 😰, never managed to replicate that effort, it has now since become my other additional goal, you see how it works now?\n\nworkout_df %&gt;%\n  ggplot(aes(y = n_workouts, x = factor(1))) +\n  geom_boxplot(width = 0.2) +\n  labs(title = \"The overal picture\",\n       x = \"\", y = \"No. of workouts\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\nFigure 2: Workout summary\n\n\n\n\nI averaged 10 workouts per month during the 10 months period (despite registering 0 in April and May) Figure 2. I registered a median of 13 workouts per month, not bad for a beginner though, proud of these numbers to the greatest heights."
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html#sec-monthviz",
    "href": "posts/celebrate-100-gym-days/index.html#sec-monthviz",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "Monthly Visual Chart",
    "text": "Monthly Visual Chart\nThe cascede images below shows monthly presence in the gym. The light-green highlighted dates represents the days I worked out for that month. July has a good pattern, I love it 🤗\n\n\n\n\n\n\nJan\n\n\n\n\n\n\n\nFeb\n\n\n\n\n\n\n\nMar\n\n\n\n\n\n\n\nApr\n\n\n\n\n\n\n\nMay\n\n\n\n\n\n\n\n\n\nJun\n\n\n\n\n\n\n\nJul\n\n\n\n\n\n\n\nAug\n\n\n\n\n\n\n\nSept\n\n\n\n\n\n\n\nOct"
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html#workout-schedule-structuring-success",
    "href": "posts/celebrate-100-gym-days/index.html#workout-schedule-structuring-success",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "Workout schedule: Structuring Success",
    "text": "Workout schedule: Structuring Success\nA structured workout schedule was instrumental in maintaining consistency. It included various exercises targeting different muscle groups and allocated specific times to each.\n\n# Work out schedule\nworkout_sched &lt;- tribble(\n  ~ Day, ~ `50 mins`, ~ `30 mins`, ~`10 mins`,\n  \"Monday\", \"Chest\", \"Biceps\", \"Treadmill Run\",\n  \"Tuesday\", \"Back\", \"Triceps\", \"Treadmill Run\",\n  \"Wednesday\", \"Legs\", \"Shoulder\", \"Treadmill Run\",\n  \"Thursday\", \"Core\", \"Compound exercises\", \"Treadmill Run\",\n  \"Friday\", NA_character_, NA_character_, \"Rest day (Not always)\",\n  \"Saturday\", NA_character_, NA_character_, \"Outdoor half-marathon 11 - 13kms\")\n\nThe table below shows the daily work-out schedule, making sure I spend more time on muscles that take long to grow and always end my workout with a 10 mins run on the treadmill. The headers are day of the week and mins allocated to each exercise.\n\nworkout_sched %&gt;% \n  qflextable()\n\n\n\nTable 1:  Workout schedule Day50 mins30 mins10 minsMondayChestBicepsTreadmill RunTuesdayBackTricepsTreadmill RunWednesdayLegsShoulderTreadmill RunThursdayCoreCompound exercisesTreadmill RunFridayRest day (Not always)SaturdayOutdoor half-marathon 11 - 13kms\n\n\n\nAs a starting point, I aimed at working out for at least 1hr 30 mins; focusing two muscles a day as shown in Table 1. Friday was not all rest day as can been seen in monthly visual chart - 3, but was mostly a plan to catch up on missed appointment a.k.a punish yourself for missing many sessions the prior month."
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html#visualizing-progress-tracking-transformations",
    "href": "posts/celebrate-100-gym-days/index.html#visualizing-progress-tracking-transformations",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "Visualizing Progress: Tracking Transformations",
    "text": "Visualizing Progress: Tracking Transformations\nVisualizing my workout data unveiled the trends in my workout duration, enabling me to track improvements and shifts in my routine.\n\nAverage monthly workout duration\n\ngym_log %&gt;% \n  group_by(month) %&gt;% \n  summarise(\n    avg_dur = mean(duration_mins)\n  ) %&gt;% \n  ggplot(aes(x = month, y = avg_dur, group = 1)) +\n  geom_line()+\n  geom_point() +\n  labs(\n    title = \"Monthly average workout duration in minutes\",\n    x = \"Month\",\n    y = \"duration (mins)\"\n  )\n\n\n\n\nFigure 3: Average workout duration\n\n\n\n\nThe month of June was my worst solo effort, but this trend shows the significance of my gym buddy the month of after June and the average stay and working out in the gym that shot up by approximately 38% and almost stabilized. I can only hope for an upward trend going forward.\n\n\nTrend - best month\nThe month after getting a workout buddy was the best. All metrics were at their all time best i.e. more days in the gym, more time working out for the days designated; and in fact, the second best monthly average workout duration this year Figure 3. This once again tells the importance of the incoming energy, support, encouragement and accountability from my workout buddy.\n\n# create metrics for dynamic labeling\ntot_mins &lt;- gym_log %&gt;% filter(month == \"Jul\") %&gt;% \n  select(duration_mins) %&gt;% pluck() %&gt;% sum()\n\n# days worked\ndays_work &lt;- gym_log %&gt;% filter(month == \"Jul\") %&gt;% nrow()\n \n\ngym_log %&gt;% \n  filter(month == \"Jul\") %&gt;% \n  ggplot(aes(x = visit_date, y = duration_mins, group = 1)) + \n  geom_point()+\n  geom_line() +\n  scale_x_date(date_breaks = \"2 days\",    # break by 1 month interval\n               #date_labels = \"%d\\n%b\",\n               labels = label_date_short()) +       # show only month of the year\n  labs(x = \"Date\", y = \"duration(mins)\",\n       title = \"Trend of best effort, July 2023\",\n       caption = str_glue(\"Days working out = {days_work}\\nTotal mins = {comma(tot_mins)}\"))\n\n\n\n\nFigure 4: Dialy work out trend, July 2023\n\n\n\n\n\n\nSummary of workout days:Evolution of Dedication\nIn general, my working out routine greatly improved months following June and this can all be attributed to having a workout buddy. The box plots below paint a picture of a transformation in the process; which can only get better with time. Amidst all the fired up effort, I missed a few days/months; but that’s a story for another day of why that happened.\n\ngym_log %&gt;% \n  ggplot(aes(x = month, y = duration_mins)) +\n  geom_boxplot(width = 0.5, outlier.color = \"red\") +\n  labs(y = \"duration(mins)\")\n\n\n\n\nFigure 5: Summary of monthly workouts\n\n\n\n\nSummarizing my monthly workouts using box plots revealed the distribution of my workout durations and provided further insights into my fitness journey Figure 5.\n\n\nHistogram of workout duration\nA histogram of workout duration showcases the diverse nature of my exercise routines, with both short, intense sessions and longer, endurance-based workouts.\nI am proud of every value on the x-axis and how many times it occurred on the y-axis. I don’t want to settle on the middle ground but I lik the extremes because they come into play when circumstance prevail.\n\ngym_log %&gt;% \n  ggplot(aes(x = duration_mins)) +\n  geom_histogram(fill = \"steelblue\") +\n  labs(x = \"duration(mins)\", y = \"Frequency\",\n       title = \"Histogram of all-time workout duration\",\n       subtitle = \"Jan upto Oct 2023\")\n\n\n\n\nFigure 6: Histogram of all-time workout duration"
  },
  {
    "objectID": "posts/celebrate-100-gym-days/index.html#conclusion-and-take-aways",
    "href": "posts/celebrate-100-gym-days/index.html#conclusion-and-take-aways",
    "title": "Celebrating 100 days of Gym dedication",
    "section": "Conclusion and take aways",
    "text": "Conclusion and take aways\nThis celebration is about a habit I have managed to pick, the growth and the transformation it has brought about.This type of a journey requires accountability, and for that reason, I have a workout buddy Tony Henry who I am accountable to, and the gym app was very crucial in tracking my progress.\nFitness journey comes with challenges, for instance having to wake up in the morning at around 6am to workout while you would rather keep warm and sleep is the hardest choice. Overcoming this type of challenge was key to my success, made me mentally stronger and enabled me to become more resilient.\nListen to your body always, know when to push harder and when to rest. Celebrate every milestone you have achieved, even if it meant you lifted 2.5kg extra on top of your regular weight limit. Celebrations makes you more energetic and looking forward to move up next level the following workout day.\nI hope my story inspires you to start your fitness journey or continue the one you already archived. It’s doable, if you put mind to it. Here is to 100 days of hard-work, dedication and determination 🙌🥳."
  },
  {
    "objectID": "publications/publications.html#authored-conference-papers",
    "href": "publications/publications.html#authored-conference-papers",
    "title": "Christopher Maronga",
    "section": "Authored conference papers",
    "text": "Authored conference papers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher Maronga",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nWeb scraping using R\n\n\n\n\n\n\n\n\n\n\n\n\n2023-10-31\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\ndplyr::coalesce() in action\n\n\n\n\n\n\n\n\n\n\n\n\n2023-10-27\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCelebrating 100 days of Gym dedication\n\n\n\n\n\n\n\n\n\n\n\n\n2023-10-24\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nWorking with Databases in R\n\n\n\n\n\n\n\n\n\n\n\n\n2021-06-12\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAutomating Clinical Data Management\n\n\n\n\n\n\n\n\n\n\n\n\n2018-12-16\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  }
]